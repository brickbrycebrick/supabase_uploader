#!/usr/bin/env python
"""
CSV/XLSX to Supabase Uploader

A CLI tool that helps beginners load data from CSV/XLSX files into Supabase.
It can generate SQL scripts for table creation and upload data to Supabase.
"""

import os
import re
import sys
import json
import argparse
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, date
from typing import Dict, List, Tuple, Optional, Union, Any
from dotenv import load_dotenv
from supabase import create_client, Client

# Load environment variables
load_dotenv()

# Get Supabase credentials from environment variables
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")


def sanitize_table_name(filename: str) -> str:
    """
    Convert a filename to a valid PostgreSQL table name by removing the extension,
    replacing spaces with underscores, and removing special characters.
    
    Args:
        filename: The filename to convert
        
    Returns:
        A sanitized table name
    """
    # Get the base name without extension
    base_name = Path(filename).stem
    
    # Replace spaces with underscores and remove special characters
    sanitized = re.sub(r'[^\w\s]', '', base_name).replace(' ', '_').lower()
    
    # Ensure the name starts with a letter or underscore
    if sanitized and not sanitized[0].isalpha() and sanitized[0] != '_':
        sanitized = f"t_{sanitized}"
        
    return sanitized


def infer_sql_data_type(series: pd.Series) -> str:
    """
    Infer the SQL data type from a pandas Series.
    
    Args:
        series: The pandas Series to infer the type from
        
    Returns:
        The corresponding SQL data type
    """
    dtype = series.dtype
    non_null_values = series.dropna()
    
    # Check if the series is empty or all null
    if non_null_values.empty:
        return "TEXT"
    
    # Check for numeric types
    if pd.api.types.is_integer_dtype(dtype):
        # Check if values fit in INTEGER range
        if non_null_values.min() >= -2147483648 and non_null_values.max() <= 2147483647:
            return "INTEGER"
        return "BIGINT"
    
    if pd.api.types.is_float_dtype(dtype):
        return "DOUBLE PRECISION"
    
    # Check for boolean
    if pd.api.types.is_bool_dtype(dtype):
        return "BOOLEAN"
    
    # Check for datetime
    if pd.api.types.is_datetime64_dtype(dtype):
        return "TIMESTAMP"
    
    # Check for date
    if isinstance(dtype, pd.PeriodDtype):
        return "DATE"
    
    # Try to detect date strings
    if dtype == 'object':
        # Check if values look like dates (YYYY-MM-DD format)
        sample = non_null_values.iloc[0] if len(non_null_values) > 0 else ""
        if isinstance(sample, str):
            # Try to parse as date
            try:
                # Check if all values match date pattern
                date_pattern = r'^\d{4}-\d{2}-\d{2}$'
                if non_null_values.str.match(date_pattern).all():
                    # Try to convert to datetime to validate
                    pd.to_datetime(non_null_values, format='%Y-%m-%d', errors='raise')
                    return "DATE"
            except (ValueError, TypeError):
                pass
    
    # Default to TEXT for everything else
    return "TEXT"


def generate_create_table_sql(df: pd.DataFrame, table_name: str) -> str:
    """
    Generate SQL to create a table based on the DataFrame schema.
    
    Args:
        df: The pandas DataFrame
        table_name: The name of the table to create
        
    Returns:
        SQL statement to create the table
    """
    columns = []
    
    # Process each column to determine its SQL type
    for col_name, col_data in df.items():
        sql_type = infer_sql_data_type(col_data)
        # Sanitize column name (replace spaces with underscores and remove special chars)
        sanitized_col_name = re.sub(r'[^\w\s]', '', col_name).replace(' ', '_').lower()
        columns.append(f'    "{sanitized_col_name}" {sql_type}')
    
    # Add an ID column as primary key if it doesn't exist
    if 'id' not in [col.lower() for col in df.columns]:
        columns.insert(0, '    "id" SERIAL PRIMARY KEY')
    
    # Create the SQL statement
    columns_str = ',\n'.join(columns)
    sql = f"""-- SQL script generated by CSV/XLSX to Supabase Uploader
-- Drop the table if it exists
DROP TABLE IF EXISTS "{table_name}";

-- Create the table with inferred schema
CREATE TABLE "{table_name}" (
{columns_str}
);
"""
    return sql


def save_sql_script(sql: str, output_path: str) -> None:
    """
    Save the SQL script to a file. Creates the directory if it doesn't exist.
    
    Args:
        sql: The SQL script to save
        output_path: The path to save the SQL script to
    """
    # Ensure the directory exists
    directory = os.path.dirname(output_path)
    if directory and not os.path.exists(directory):
        os.makedirs(directory)
        
    with open(output_path, 'w') as f:
        f.write(sql)
    print(f"SQL script saved to {output_path}")


def upload_to_supabase(df: pd.DataFrame, table_name: str, mode: str = "overwrite") -> Tuple[bool, str]:
    """
    Upload the DataFrame to Supabase.
    
    Args:
        df: The pandas DataFrame to upload
        table_name: The name of the table to upload to
        mode: Upload mode - "overwrite" (default) or "insert"
        
    Returns:
        A tuple of (success, message)
    """
    if not SUPABASE_URL or not SUPABASE_KEY:
        return False, "Supabase credentials not found. Please set SUPABASE_URL and SUPABASE_KEY environment variables."
    
    try:
        # Initialize Supabase client
        supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
        
        # Check if the table exists
        try:
            # Try to get the first row to check if table exists
            table_check = supabase.table(table_name).select("*").limit(1).execute()
        except Exception as e:
            if "relation" in str(e) and "does not exist" in str(e):
                return False, f"Table '{table_name}' does not exist in Supabase. Please create it first using the generated SQL script."
            else:
                # Some other error occurred
                return False, f"Error checking table existence: {str(e)}"
        
        # Convert DataFrame to records and handle non-serializable types
        # First, replace NaN values with None for proper JSON serialization
        df = df.replace({np.nan: None})
        
        # Custom converter for datetime objects
        def convert_to_serializable(obj):
            if isinstance(obj, (datetime, pd.Timestamp)):
                return obj.isoformat()
            elif isinstance(obj, date):
                return obj.isoformat()
            return obj
        
        # Apply conversion to each element in the DataFrame
        for col in df.columns:
            if pd.api.types.is_datetime64_dtype(df[col].dtype):
                df[col] = df[col].apply(lambda x: convert_to_serializable(x) if x is not None else None)
        
        # Convert DataFrame to records (list of dicts)
        records = df.to_dict(orient='records')
        
        # Upload the data in batches to avoid request size limits
        batch_size = 1000
        total_records = len(records)
        uploaded_count = 0
        
        for i in range(0, total_records, batch_size):
            batch = records[i:i+batch_size]
            
            if mode == "overwrite" and i == 0:
                # For the first batch in overwrite mode, delete all existing data first
                try:
                    print(f"Deleting all existing data from {table_name}...")
                    # Use a SQL query to truncate the table instead of DELETE
                    # This is more reliable and doesn't require a WHERE clause
                    truncate_query = f"TRUNCATE TABLE \"{table_name}\" RESTART IDENTITY;"
                    
                    # Execute the truncate query using RPC
                    try:
                        # First try using the execute_sql RPC function if available
                        supabase.rpc('execute_sql', {'query': truncate_query}).execute()
                    except Exception as rpc_error:
                        # If RPC fails, try using a raw SQL query via the REST API
                        try:
                            # Use a filter that will match all records as a workaround
                            # This is not ideal but works as a fallback
                            supabase.table(table_name).delete().filter('id', 'gte', 0).execute()
                        except Exception as filter_error:
                            # If that fails too, try one more approach with a different filter
                            try:
                                # Try with a different column that might exist
                                for col in df.columns:
                                    try:
                                        # Try with this column
                                        supabase.table(table_name).delete().filter(col, 'is', None).or_(f'{col}.not.is', None).execute()
                                        # If we get here, it worked
                                        break
                                    except:
                                        # Try the next column
                                        continue
                            except Exception as e:
                                # If all approaches fail, just continue with insert
                                print(f"Warning: Could not delete records with any approach: {str(e)}")
                except Exception as e:
                    print(f"Warning: Could not delete all records: {str(e)}")
                    print("Continuing with insert operation...")
                
                # Insert the batch after deletion
                result = supabase.table(table_name).insert(batch).execute()
                print(f"Inserted {uploaded_count + len(batch)}/{total_records} records...")
            else:  # mode == "insert"
                # Use insert (will fail if records already exist)
                result = supabase.table(table_name).insert(batch).execute()
                print(f"Inserted {uploaded_count + len(batch)}/{total_records} records...")
                
            uploaded_count += len(batch)
        
        operation_map = {
            "insert": "inserted",
            "overwrite": "overwritten and inserted"
        }
        operation = operation_map.get(mode, "uploaded")
        return True, f"Successfully {operation} {total_records} records to {table_name}"
    
    except Exception as e:
        return False, f"Error uploading to Supabase: {str(e)}"


def read_file(file_path: str) -> Tuple[Optional[pd.DataFrame], Optional[str]]:
    """
    Read a CSV or XLSX file into a pandas DataFrame.
    
    Args:
        file_path: Path to the CSV or XLSX file
        
    Returns:
        A tuple of (DataFrame, error_message)
    """
    try:
        file_extension = Path(file_path).suffix.lower()
        
        if file_extension == '.csv':
            df = pd.read_csv(file_path)
        elif file_extension in ['.xlsx', '.xls']:
            # Parse dates when reading Excel files
            df = pd.read_excel(file_path, parse_dates=True)
        else:
            return None, f"Unsupported file format: {file_extension}. Please use CSV or XLSX."
        
        # Check if the DataFrame is empty
        if df.empty:
            return None, "The file contains no data."
        
        return df, None
    
    except Exception as e:
        return None, f"Error reading file: {str(e)}"


def main():
    """Main function to run the CLI tool."""
    parser = argparse.ArgumentParser(
        description="Upload CSV/XLSX files to Supabase and generate SQL scripts."
    )
    
    parser.add_argument(
        "file_path",
        help="Path to the CSV or XLSX file"
    )
    
    parser.add_argument(
        "--generate-sql",
        action="store_true",
        help="Generate SQL script for table creation"
    )
    
    parser.add_argument(
        "--upload",
        action="store_true",
        help="Upload data to Supabase"
    )
    
    parser.add_argument(
        "--table-name",
        help="Custom table name (defaults to sanitized filename)"
    )
    
    parser.add_argument(
        "--sql-output",
        help="Path to save the generated SQL script (defaults to sql_scripts/<table_name>.sql)"
    )
    
    parser.add_argument(
        "--insert-only",
        action="store_true",
        help="Use insert instead of overwrite (default is overwrite which replaces all existing data)"
    )
    
    args = parser.parse_args()
    
    # Read the file
    df, error = read_file(args.file_path)
    if error:
        print(f"Error: {error}")
        sys.exit(1)
    
    # Get table name
    table_name = args.table_name if args.table_name else sanitize_table_name(args.file_path)
    print(f"Using table name: {table_name}")
    
    # Generate SQL if requested
    if args.generate_sql:
        sql = generate_create_table_sql(df, table_name)
        
        # Determine output path for SQL (default to sql_scripts folder)
        if args.sql_output:
            sql_output = args.sql_output
        else:
            sql_output = os.path.join("sql_scripts", f"{table_name}.sql")
            
        save_sql_script(sql, sql_output)
    
    # Upload to Supabase if requested
    if args.upload:
        if not args.generate_sql:
            print("Warning: Uploading without generating SQL first. Make sure the table exists in Supabase.")
        
        # Determine upload mode based on flags
        mode = "insert" if args.insert_only else "overwrite"
        print(f"Using {mode} operation for data upload...")
        
        success, message = upload_to_supabase(df, table_name, mode=mode)
        print(message)
        
        if not success:
            sys.exit(1)
    
    # If neither option was selected, show help
    if not args.generate_sql and not args.upload:
        parser.print_help()
        print("\nError: Please specify at least one action (--generate-sql or --upload)")
        sys.exit(1)


if __name__ == "__main__":
    main()
